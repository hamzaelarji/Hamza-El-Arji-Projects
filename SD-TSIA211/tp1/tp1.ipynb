{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD-TSIA 211\n",
    "# Optimization for Machine Learning\n",
    "\n",
    "# Computer lab : Letâ€™s reverse-engineer the data center "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By Hamza El Arji**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 :  Database and statistical model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)\n",
    "\n",
    "# Constructing matrices for min_w ||A w - b||_2**2\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0) # Calculate mean of training data matrix along each column\n",
    "M = data_matrix_train - matrix_mean # Center the training data matrix by subtracting the mean\n",
    "matrix_std = np.std(M, axis=0) # Calculate standard deviation of centered data matrix along each column\n",
    "M = M / matrix_std # Normalize the centered data matrix by dividing by standard deviation\n",
    "\n",
    "# Construct matrix A and vector b for the training set\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T])\n",
    "b = COP_train[:,3]\n",
    "\n",
    "# Constructing matrices for the test set\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test = np.hstack([M_test, np.ones((M_test.shape[0],1)), -(M_test.T * COP_test[:,3]).T])\n",
    "b_test = COP_test[:,3]\n",
    "\n",
    "\n",
    "# Loading raw data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('Raw_Dataset_May.csv')\n",
    "\n",
    "def name_to_subcategory_and_details(col_name):\n",
    "    if np.isreal(col_name):\n",
    "        col_name = names[col_name]\n",
    "    indices = np.nonzero((data['NAME'] == col_name).values)[0]\n",
    "    if len(indices) > 0:\n",
    "        subcategory = data['SUBCATEGORY'].iloc[[indices[0]]].values[0]\n",
    "        details = data['DETAILS'].iloc[[indices[0]]].values[0]\n",
    "        return subcategory, details\n",
    "    else:\n",
    "        print('unknown name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 :  Least squares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 3.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To demonstrate that if** \\(Aw=b\\), **then**\n",
    "$ y(t) = \\frac{w_1^T \\tilde{x}(t)  + w_0}{w_2^T \\tilde{x}(t)  + 1} $ \n",
    "\n",
    "**Let's consider the expression for** $ (Aw)_t $ **and simplify it:**\n",
    "$ (Aw)_t = \\tilde{x}(t)^T w_1 + w_0 - y(t) \\times \\tilde{x}(t)^T w_2 $ \n",
    "\n",
    "**Given that** \\(Aw = b\\), **we can replace** $(Aw)_t $ **with** $(b_t = y(t))$:\n",
    "$  y(t) =  \\tilde{x}(t)^T w_1 + w_0 - y(t) \\times \\tilde{x}(t)^T w_2  $\n",
    "\n",
    "**So, we have:**\n",
    "$ y(t) = \\frac{w_1^T \\tilde{x}(t)  + w_0}{w_2^T \\tilde{x}(t)  + 1} $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 3.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Parameters:\n",
      "[-0.00927821  0.08309371 -0.03672704 ...  0.01980595 -0.03057174\n",
      " -0.01188614]\n"
     ]
    }
   ],
   "source": [
    "# Solve the OLS problem\n",
    "w_optimal, residuals, _, _ = np.linalg.lstsq(A, b, rcond=None)\n",
    "print(\"Optimized Parameters:\")\n",
    "print(w_optimal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 3.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 780.8984793523564\n"
     ]
    }
   ],
   "source": [
    "# compute the predicted values and the MSE\n",
    "predicted_values =A_test @ w_optimal\n",
    "mse = np.mean((predicted_values - b_test) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 3.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01238313  0.05780406 -0.00127775 ...  0.01590558 -0.03567897\n",
      "  0.0131232 ]\n",
      "Unregularized Mean Squared Error: 780.8984793523564\n",
      "Regularized Mean Squared Error: 301.0548280940246\n"
     ]
    }
   ],
   "source": [
    "# Regularization parameter\n",
    "lambda_val = 100\n",
    "\n",
    "# Construct the regularized matrices\n",
    "A_reg = np.vstack([A, np.sqrt(lambda_val) * np.eye(A.shape[1])])\n",
    "b_reg = np.concatenate([b, np.zeros(A.shape[1])])\n",
    "\n",
    "# Solve the regularized least squares problem\n",
    "w_optimal_reg, residuals_reg, _, _ = np.linalg.lstsq(A_reg, b_reg, rcond=None)\n",
    "print(w_optimal_reg)\n",
    "\n",
    "# Predict the values using the optimized parameters\n",
    "predicted_values_reg = np.dot(A_test, w_optimal_reg)\n",
    "\n",
    "# Calculate the mean squared error for the regularized problem\n",
    "mse_reg = np.mean((predicted_values_reg - b_test) ** 2)\n",
    "\n",
    "# Print the MSE for both problems\n",
    "print(\"Unregularized Mean Squared Error:\", mse)\n",
    "print(\"Regularized Mean Squared Error:\", mse_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 3.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function is convex.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the gradient and the hessian matrice of f1\n",
    "gradient_f1 = np.dot(A.T, np.dot(A, w_optimal_reg) - b) + lambda_val *w_optimal_reg\n",
    "hessian_f1 = np.dot(A.T, A) + lambda_val * np.eye(A.shape[1])\n",
    "\n",
    "# Is the function convex ?\n",
    "is_convex = np.all(np.linalg.eigvals(hessian_f1) >= 0)\n",
    "if is_convex:\n",
    "    print(\"The function is convex.\")\n",
    "else:\n",
    "    print(\"The function is not convex.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 3.6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 1e-07\n",
      "Number of iterations: 110000\n",
      "Gradient norm: 5.0435923808453795\n",
      "Gradient optimal: 1.313600338502328e-10\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables for gradient descent\n",
    "w = np.zeros(A.shape[1])\n",
    "step_size = 0.0000001\n",
    "convergence_threshold = 1\n",
    "iterations = 0\n",
    "positive_infinity = float('inf')\n",
    "gradient_norm = positive_infinity\n",
    "\n",
    "# Perform gradient descent\n",
    "while gradient_norm > convergence_threshold and iterations<110000:\n",
    "    gradient = 2 * np.dot(A.T, np.dot(A, w) - b) + lambda_val * w\n",
    "    w -= step_size * gradient\n",
    "    gradient_norm = np.linalg.norm(gradient)\n",
    "    iterations += 1\n",
    "\n",
    "# Print the step size and the number of iterations\n",
    "print(\"Step size:\", step_size)\n",
    "print(\"Number of iterations:\", iterations)\n",
    "print(\"Gradient norm:\", gradient_norm)\n",
    "print(\"Gradient optimal:\", np.linalg.norm(gradient_f1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4 :  Regularization for a sparse model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 4.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To formulate the objective function  $ F_2 $ for the Lasso problem, we need to decompose it into two parts: a differentiable part $ f_2 $ and a non-differentiable part $ g_2 $. The proximal operator of $ g_2 $ should be easy to compute.\n",
    "\n",
    "The Lasso objective function is defined as follows:\n",
    "\n",
    "$ F_2(w) = \\frac{1}{2} \\|Aw - b\\|_2^2 + \\lambda \\|w\\|_1 $\n",
    "\n",
    "Now, let's decompose it into $ f_2 $ and $ g_2 $:\n",
    "\n",
    "$  f_2(w) = \\frac{1}{2} \\|Aw - b\\|_2^2 $   and  $ g_2(w) = \\lambda \\|w\\|_1 $\n",
    "\n",
    "The proximal operator $ \\text{prox}_{g_2}(v) $ is defined as:\n",
    "\n",
    "$ \\text{prox}_{g_2}(v) = \\arg\\min_w \\left( g_2(w) + \\frac{1}{2} \\|v-w\\|_2^2 \\right) $\n",
    "\n",
    "Now, let's calculate the gradient of $ f_2(w) $:\n",
    "\n",
    "$ \\nabla f_2(w) = A^T(Aw - b) $\n",
    "\n",
    "This is because the gradient of the quadratic term $ \\frac{1}{2} \\|Aw - b\\|_2^2 $ with respect to $ w $ is $ A^T(Aw - b) $.\n",
    "\n",
    "So, in summary:\n",
    "\n",
    "$ F_2(w) = f_2(w) + g_2(w) \\\\ $ \n",
    "$ f_2(w) = \\frac{1}{2} \\|Aw - b\\|_2^2 \\\\ $\n",
    "$ g_2(w) = \\lambda \\|w\\|_1 \\\\$\n",
    "$  \\nabla f_2(w) = A^T(Aw - b) \\\\$\n",
    "\n",
    "This formulation allows us to apply proximal gradient methods or other optimization algorithms that can handle proximal operators for solving Lasso problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 4.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weight Vector:\n",
      "[-0.00406567  0.01906334  0.01028991 ...  0.00188562 -0.00609072\n",
      " -0.00041177]\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function\n",
    "def objective_function(A, b, w, lambda_val):\n",
    "    return 0.5 * np.linalg.norm(np.dot(A, w) - b)**2 + lambda_val * np.linalg.norm(w, 1)\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient(A, b, w):\n",
    "    return np.dot(A.T, np.dot(A, w) - b)\n",
    "\n",
    "# Define the proximal operator for L1 regularization\n",
    "def proximal_operator(w, lambda_val, step_size):\n",
    "    return np.sign(w) * np.maximum(np.abs(w) - lambda_val * step_size, 0)\n",
    "\n",
    "# Set the parameters\n",
    "lambda_val = 200\n",
    "step_size = 1e-07\n",
    "max_iterations = 1000\n",
    "\n",
    "# Initialize the weight vector\n",
    "w = np.zeros(A.shape[1])\n",
    "\n",
    "# Perform proximal gradient descent\n",
    "for iteration in range(max_iterations):\n",
    "    # Calculate the gradient\n",
    "    grad = gradient(A, b, w)\n",
    "    \n",
    "    # Update the weight vector using proximal operator\n",
    "    w = proximal_operator(w - step_size * grad, lambda_val * step_size, step_size)\n",
    "    \n",
    "# Print the optimized weight vector\n",
    "print(\"Optimized Weight Vector:\")\n",
    "print(w)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**question 4.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time with Fixed Step Size: 1.2745351791381836\n",
      "Execution Time with Line Search: 87.93779373168945\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(A, b, w, lambda_val):\n",
    "    return 0.5 * np.linalg.norm(np.dot(A, w) - b)**2 + lambda_val * np.linalg.norm(w, 1)\n",
    "\n",
    "# Define the gradient of the objective function\n",
    "def gradient(A, b, w):\n",
    "    return np.dot(A.T, np.dot(A, w) - b)\n",
    "\n",
    "# Define the proximal operator for L1 regularization\n",
    "def proximal_operator(w, lambda_val, step_size):\n",
    "    return np.sign(w) * np.maximum(np.abs(w) - lambda_val * step_size, 0)\n",
    "\n",
    "# Set the parameters\n",
    "lambda_val = 200\n",
    "max_iterations = 1000\n",
    "\n",
    "# Initialize the weight vector\n",
    "w = np.zeros(A.shape[1])\n",
    "\n",
    "# Perform proximal gradient descent with fixed step size\n",
    "start_time_fixed = time.time()\n",
    "for iteration in range(max_iterations):\n",
    "    # Calculate the gradient\n",
    "    grad = gradient(A, b, w)\n",
    "    \n",
    "    # Update the weight vector using proximal operator\n",
    "    w = proximal_operator(w - step_size * grad, lambda_val * step_size, step_size)\n",
    "    \n",
    "end_time_fixed = time.time()\n",
    "\n",
    "# Perform proximal gradient descent with line search\n",
    "w = np.zeros(A.shape[1])\n",
    "step_size = 1e-07\n",
    "start_time_line_search = time.time()\n",
    "for iteration in range(max_iterations):\n",
    "    # Calculate the gradient\n",
    "    grad = gradient(A, b, w)\n",
    "    \n",
    "    # Perform line search to find the optimal step size\n",
    "    t = 1.0\n",
    "    while objective_function(A, b, w - t * grad, lambda_val) > objective_function(A, b, w, lambda_val) - 0.5 * t * np.linalg.norm(grad)**2:\n",
    "        t *= 0.5\n",
    "    \n",
    "    # Update the weight vector using proximal operator with the optimal step size\n",
    "    w = proximal_operator(w - t * grad, lambda_val * t, t)\n",
    "    \n",
    "end_time_line_search = time.time()\n",
    "\n",
    "# Calculate the execution time for both approaches\n",
    "execution_time_fixed = end_time_fixed - start_time_fixed\n",
    "execution_time_line_search = end_time_line_search - start_time_line_search\n",
    "\n",
    "# Print the execution times\n",
    "print(\"Execution Time with Fixed Step Size:\", execution_time_fixed)\n",
    "print(\"Execution Time with Line Search:\", execution_time_line_search)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6 :  Comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimized Weight Vector for l1:** \n",
    "\n",
    "**[-0.00406567  0.01906334  0.01028991 ...  0.00188562 -0.00609072 -0.00041177]** \n",
    " \n",
    "**Optimized Weight Vector for l2:**\n",
    "\n",
    " **[-0.01238313  0.05780406 -0.00127775 ...  0.01590558 -0.03567897 0.0131232 ]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 7 :  Commenting 2 reports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
